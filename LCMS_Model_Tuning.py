#--------------------------------------------------------------------------
#                    lcms_tuning.py
#--------------------------------------------------------------------------
# for tuning of Random Forest model and evaluation of timesync sample size

import pandas as pd
import numpy as np
import os, glob, csv, pdb
from datetime import datetime
import matplotlib
#matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, cross_val_predict, learning_curve, ShuffleSplit, GridSearchCV, train_test_split, cross_validate, StratifiedKFold, GroupKFold
from sklearn.metrics import confusion_matrix, precision_recall_curve, cohen_kappa_score, precision_score, recall_score 
from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score
from sklearn.utils import parallel_backend
from sklearn.feature_selection import RFECV
from __init__ import *
from Modules import accuracy_and_sampling_lib as accuracySamplingLib
from Modules import lcmsLib 
from Modules.LCMSVariables import *


#-----------------------
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# LESSONS FROM v2021-7 Implementation!!!!!! Need to edit these things before running this again.
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#-----------------------
'''
1) A large mtry will cause the runs to take a really long time to run. We ended up having to change the mtry for land use and gain from 14 and 10 to 6 in order to
get them to run in a reasonable time. Make sure to limit the mtry in the tuning below moving forward.
2) For multi-class runs like land use and land cover, a minLeafPopulation of 1 can make the tree too complex 
and result in this error: Image.classify: Computed value is too large
We had to increase this parameter to 11 for land cover and land use to get it to run in GEE. It may not need to be that big?? but it needs to be bigger than 1.
'''


#------ PATHS AND STUDY AREA ------
studyArea = 'CONUS'
run_name = 'v2021_standard_tuning' # - this creates a separate folder under "other_output" for this runname, but only appends the run_name 
    # to the tuning summary file name. This can be left blank ('') if a separate runname is not wanted.

modelingDict = paramDict[studyArea]['Modeling']
trainingDict = paramDict[studyArea]['Training_Data']

#------ RUN OPTIONS ------
# Model options 
#These models will vary depending on AOI
#If running LC and LU models, run DOM_LC and DOM_LU first to get summary tables used in tunning LC and LU models 
#models = modelingDict['changeClasses']
#models = modelingDict['landcoverClasses']
#models = modelingDict['landuseClasses']
models = ['DOM_SEC_LC'] # 'DOM_LC','DOM_LU','DOM_SEC_LC'

# Model tuning 
runCorrelatedVariables = False # This only needs to be run once for each run_name
runVariableSelection = False
runParameterTuning = True
plotLearningCurves = False

# Run after model tuning 
# If rfDecisionTree = True, other tuning = False (runCorrelatedVariables, runVariableSelection, runParameterTuning, plotLearningCurves) 
rfDecisionTree = False

n_jobs = 8
remove_lowconfidence = modelingDict['removeLowConfidence']
onlyDomClasses = False # This is only for multi-class DOM_LC model - whether to run only the dominant classes (Tree, Grass, Shrub etc.) or include the mixed primary-secondary classes (Tree-Grass, Tree-Shrub etc.)
changeProcessOnlyFastLoss = True # whether to only apply change process model over fast loss pixels/plots

landcoverClasses = modelingDict['landcoverClasses']
landuseClasses = modelingDict['landuseClasses']
changeClasses = modelingDict['changeClasses']
includeCompositePredictors= modelingDict['includeCompositePredictors']
useTunedTrees = modelingDict['useTunedTrees']

#------ PATHS ------
indir = modelingDict['trainingTableFolder']
strataFile = modelingDict['timesyncStrataFile']
strataDict = modelingDict['strataDict']
rootdir = trainingDict['localTrainingDataPath']
tuningRootDir = os.path.join(rootdir,'TuningParameters') 
otherOutputDir = os.path.join(tuningRootDir,'other_output')
rundir = os.path.join(otherOutputDir, run_name)
variableSelectionOutdir = os.path.join(rundir,'variableSelection')
correlatedVarsDir = os.path.join(rundir,'correlatedVariables')
learningdir = os.path.join(rundir,'learningCurves')
summarydir = os.path.join(tuningRootDir, 'Summaries')
tuningOutdir = os.path.join(rundir,'tuningParameters')
decTreeOutdir = modelingDict['DecisionTreeOutput']

#-----VARIABLE SELECTION OPTIONS------
# ***If runVariableSelection == False:
featureMethod = 'readFromFile' # 'all' or 'readFromFile' - read from list generated by variable selection process
# If featureMethod == 'readFromFile':
# model name and scoring metric will be added below
featureFileSuffix = '_'+run_name+'_Variable_Selection_'

#-----PARAMETER TUNING OPTIONS---------
# Min samples leaf population - need at least 5 for DOM_SEC_LC multiprobability in GEE, normally we did 1.
min_samples_leaf = [5,7,9,11]
#------ LEARNING CURVE OPTIONS------
# If plotLearningCurves == True and runParameterTuning == False:
estimatorParamMethod = 'default' # 'default' or 'readFromFile'
# If estimatorParamMethod == 'readFromFile'
# model name will be added beow
estimatorParamFileSuffix = '_'+run_name+'_Tuning_Output_'


#------ OTHER OPTIONS------
landcoverNameCrosswalk = {'BARREN': 'Barren',
    'GRASS': 'Grass/forb/herb',
    'IMPERVIOUS': 'Impervious',
    'SHRUBS': 'Shrubs',
    'SNOW': 'Snow/ice',
    'TREES': 'Trees',
    'TS': 'TallShrubs',
    'WATER': 'Water'}

#------ PRINT SELECTED OPTIONS------
print('Run_name:', run_name)
print('studyArea:', studyArea)
print('models:', models)

print('runVariableSelection: '+str(runVariableSelection))
if runVariableSelection == False:
    print('featureMethod: '+featureMethod)
    if featureMethod == 'readFromFile':
        print('featureFileSuffix: '+featureFileSuffix)
print('runParameterTuning: '+str(runParameterTuning))
print('plotLearningCurves: '+str(plotLearningCurves))
if plotLearningCurves == True and runParameterTuning == False:
    print('estimatorParamMethod: '+estimatorParamMethod)
    if estimatorParamMethod == 'readFromFile':
        print('estimatorParamFile: '+estimatorParamFileSuffix)

#--------------------------------------------------------------------------
#                    Initialize data and variables
#--------------------------------------------------------------------------
# Make sure directories exist
if not os.path.exists(tuningRootDir):
    os.mkdir(tuningRootDir)
for dirname in [indir,otherOutputDir,rundir,summarydir,variableSelectionOutdir,correlatedVarsDir,learningdir,tuningOutdir]:
    if not os.path.exists(dirname):
        os.mkdir(dirname)

# Read in data table
print('Reading in Data')
tables = []

for file in os.listdir(indir):
    tables.append(pd.read_csv(os.path.join(indir, file)))
trainingData = pd.concat(tables, ignore_index = True)

# Drop any plots with NaNs
allNames = trainingData.columns

if includeCompositePredictors == True:
    indNames = [name for name in allNames if (('_LT_' in name) or ('_Comp_' in name) or ('_CCDC_' in name) or ('terrain' in name) or ('temp' in name))] 
else:
    indNames = [name for name in allNames if (('_LT_' in name) or ('_CCDC_' in name) or ('terrain' in name))] # 'LT' covers both regular LT and temp_LT

trainingData.dropna(axis=0, subset=indNames, inplace=True)

# Remove Low Confidence Plots
if remove_lowconfidence:
    trainingData = trainingData[~(trainingData['COMMENTS'].str.contains('[LC]|shrub or grass dominant?', na=False) & ~trainingData['COMMENTS'].str.contains('LCR', na=False))]

# Rename DND & RNR
trainingData = trainingData.rename(columns = {'DND':'Loss','RNR':'Gain','DND_Slow':'Slow_Loss','DND_Fast':'Fast_Loss'})

# Get Strata:
if not modelingDict['strataFileStratColumn'] in trainingData.columns:
    strataTable = pd.read_csv(strataFile)
    if modelingDict['strataFileIDColumn'] != 'PLOTID':
        strataTable['PLOTID'] = strataTable[modelingDict['strataFileIDColumn']]
    trainingData = trainingData.merge(strataTable[['PLOTID', modelingDict['strataFileStratColumn']]], how='inner', on='PLOTID', copy=False)
strata = trainingData[modelingDict['strataFileStratColumn']]

#--------------------------------------------------------------------------
#                  Remove Highly Correlated Variables
#--------------------------------------------------------------------------
# This only needs to be run once for each run_name
if runCorrelatedVariables:
    print('Finding Correlated Variables')

    # Get independent variables from training data table    
    x = trainingData[indNames]

    # # reverse order of columns so that Comp features come first for each index so they will be dropped before LT features when correlated
    # x = x[x.columns[::-1]]

    # Find initial correlation of each feature to the others
    corr_matrix = x.corr().abs()
    # Calculate average correlation score of each variable
    corr_score = [corr_matrix[variable].mean() for variable in indNames]
    corr_score = zip(corr_score,indNames)
    sorted_scores = sorted(corr_score, reverse=True)
    sorted_indNames = [element for _, element in sorted_scores]

    # Redo the correlation matrix with the new order of variable names
    x = x[sorted_indNames]
    corr_matrix = x.corr().abs()
    corr_matrix.to_csv(os.path.join(correlatedVarsDir, run_name+'_CorrelationMatrix.csv'), index = False)

    # Select lower triangle of correlation matrix
    upper = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k=-1).astype(np.bool))
    
    # This matrix is ordered in order of correlation score. Use this to check. So we drop in order of correlation score.
    # upper['Average_Scores'] = [pd.concat([upper[varName],upper.loc[varName].transpose()]).mean(skipna=True) for varName in upper.columns]

    # Find index of feature columns with correlation greater than 0.95
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]

    pd.Series(to_drop).to_csv(os.path.join(correlatedVarsDir, run_name+'_DroppedCorrelatedVariables.csv'), header = False, index = False)

#--------------------------------------------------------------------------
#               Loop through Models And Apply Tuning
#--------------------------------------------------------------------------

for model in models:
    
    #-------------------------------Initial Set Up-----------------------------------------------------------
    print('Running Tuning for '+model)

    # Scoring metric: accuracy, balanced_accuracy, or f1_score
    # Multi-class models:
    if model in ['DOM_LC','DOM_SEC_LC','DOM_LU','Change_Process']: 
        scoring_metric = 'accuracy'
        multiclass = True
    # One-class models:
    else:
        scoring_metric = 'f1'
        multiclass = False
    # Nice explanation of the uses of balanced accuracy vs F1:
    # https://datascience.stackexchange.com/questions/73974/balanced-accuracy-vs-f1-score

    # Variable Selection File
    featureFile = os.path.join(variableSelectionOutdir, model + featureFileSuffix + scoring_metric+'.csv')
    print("featureFile:", featureFile)   
        
    # Tuning Parameters file
    estimatorParamFile = os.path.join(tuningOutdir, model + estimatorParamFileSuffix + scoring_metric+'.txt')
    summaryFile = os.path.join(summarydir, 'Tuning_Summary_'+studyArea+'_'+model+'_'+run_name+'.csv') 

    # get names of independent variables to include
    # Read in variables from previously generated variable selection file:
    if runVariableSelection == False and featureMethod == 'readFromFile':
        feature_impt = pd.read_csv(featureFile)
        indNames = feature_impt['Variable Name']
        print("indNames:", indNames)
    # Create new variable selection file:
    else:
        allNames = trainingData.columns
        
        if includeCompositePredictors == True:
            indNames = [name for name in allNames if (('_LT_' in name) or ('_Comp_' in name) or ('_CCDC_' in name) or ('terrain' in name))] 
        else:
            indNames = [name for name in allNames if (('_LT_' in name) or ('_CCDC_' in name) or ('terrain' in name))]         

        # Drop Correlated Variables first, if we ran it earlier then to_drop is already saved and we don't need to read it in.
        if runCorrelatedVariables == False:
            to_drop = pd.read_csv(os.path.join(correlatedVarsDir, run_name+'_DroppedCorrelatedVariables.csv'), header = None).to_numpy()
        indNames = [ind for ind in indNames if ind not in to_drop]
        print('Correlated Variables', to_drop)

    # We decided not to include fourth, fifth, sixth Comp raw ever. I forget why.
    indNames = [ind for ind in indNames if ind not in ['fourth_Comp_raw','fifth_Comp_raw','sixth_Comp_raw']]    

    # Make new binary columns for separate landuse/landcover classes
    if model in landuseClasses:
        trainingData[model] = trainingData['DOM_LU'].eq(model).astype(int)
    # elif model == 'DOM_LC' and onlyDomClasses:
    #     trainingData[model] = trainingData['DOM_LC'].eq(landcoverNameCrosswalk[model]).astype(int)   

    # Combine Barren and Impervious classes
    if model == 'BARREN-IMP':
        trainingData[model] = trainingData['BARREN'].add(trainingData['IMPERVIOUS'])

    # Create column for Stable Class ( this is generally not used)
    if model == 'STABLE':
        trainingData['STABLE'] = trainingData['Loss'].add(trainingData['Gain']).subtract(1).abs()

    # Create column with mixed land cover classes
    if model == 'DOM_SEC_LC':
        trainingData['BARREN-IMP'] = trainingData['BARREN'].add(trainingData['IMPERVIOUS'])
        dom_sec_lc_index = np.argmax(trainingData[landcoverClasses].to_numpy(), axis = 1)
        trainingData['DOM_SEC_LC'] = [landcoverClasses[i] for i in dom_sec_lc_index]  

    # Remap Change Process to new (Aug. 2021) categories
    if model == 'Change_Process':
        if studyArea == 'PR_USVI':
            cpDict = {\
                'Stable': 1,
                'Fire': 2,
                'Harvest': 2,
                'Debris': 2,
                'Hydrology': 2,
                'Mechanical': 2,
                'Wind/Ice': 3,
                'Spectral Decline': 2,
                'Structural Decline': 2,
                'Growth/Recovery': 4}  
        else:
            cpDict = {\
                'Stable': 'Stable',
                'Fire': 'Fire',
                'Harvest': 'Harvest',
                'Debris': 'Other',
                'Hydrology': 'Other',
                'Mechanical': 'Other',
                'Wind/Ice': 'Other',
                'Spectral Decline': 'Slow',
                'Structural Decline': 'Slow',
                'Growth/Recovery': 'Gain',
                'Acute Decline': 'Slow'}
    
        trainingData['Change_Process'] = trainingData['CP'].replace(cpDict)
        
        if changeProcessOnlyFastLoss:
            trainingData = trainingData[trainingData['Change_Process'].isin(['Fire','Harvest','Other'])]

    # Separate independent and dependent data
    inddata = trainingData[indNames] 
    depdata = trainingData[model]
    groups = trainingData['PLOTID'].squeeze()
    x, y = inddata, depdata.squeeze()
    
    #--------------------------------------------------------------------------
    #                  Variable Selection
    #--------------------------------------------------------------------------
    
    if runVariableSelection:

        print('Running Variable Selection')

        # train initial random forest to get feature importances
        print('Training Random Forest Classifier')
        start_time = datetime.now()

        rnd_clf = RandomForestClassifier(n_estimators = 50, n_jobs = n_jobs, oob_score = True, verbose = 0)

        # Recursive Feature Elimination
        print('Applying Recursive Feature Elimination')
        gridStep = 2
        minFeatures = 6
        variables = x.columns
        gkf = GroupKFold(n_splits=5)
        rfecvDict = {}
        varSizeList = range(len(x.columns), minFeatures, gridStep*-1)
        for numVariables in varSizeList:
            print('Number of Variables: '+str(numVariables))
            rnd_clf = RandomForestClassifier(n_estimators = 50, n_jobs = n_jobs, oob_score = True, verbose = 1)

            rfecvDict[str(numVariables)] = {}
            rfecvDict[str(numVariables)]['variables'] = variables

            foldNum = 1
            for train_index, test_index in gkf.split(trainingData, trainingData, groups):
                rfecvDict[str(numVariables)][str(foldNum)] = {}
                print('Fold '+str(foldNum))

                gk_strata_train, gk_strata_test = strata.iloc[train_index], strata.iloc[test_index]
                gkx_train, gkx_test = x[variables].iloc[train_index], x[variables].iloc[test_index]
                gky_train, gky_test = y.iloc[train_index], y.iloc[test_index]

                rnd_clf.fit(gkx_train, gky_train)
                gky_pred = rnd_clf.predict(gkx_test)

                rfecvDict[str(numVariables)][str(foldNum)]['gky_test'] = gky_test.to_numpy()
                rfecvDict[str(numVariables)][str(foldNum)]['gky_pred'] = gky_pred
                rfecvDict[str(numVariables)][str(foldNum)]['var_impt' ] = rnd_clf.feature_importances_

                foldNum += 1

            # Save average accuracy metrics
            # Here we aggregate the predictions from each fold all together, and calculate the scores from that, following suggestions in Forman and Scholz 
            # that this produces the "most unbiased" estimate of generalization performance using k-fold cross-validation: https://sebastianraschka.com/faq/docs/computing-the-f1-score.html
            gky_test = np.concatenate([rfecvDict[str(numVariables)][str(f)]['gky_test'] for f in range(1, foldNum)])
            gky_pred = np.concatenate([rfecvDict[str(numVariables)][str(f)]['gky_pred'] for f in range(1, foldNum)])

            rfecvDict[str(numVariables)]['accuracy'] = accuracy_score(gky_test, gky_pred) 
            rfecvDict[str(numVariables)]['balanced_accuracy'] = balanced_accuracy_score(gky_test, gky_pred)  
            if multiclass: 
                rfecvDict[str(numVariables)]['f1'] = f1_score(gky_test, gky_pred, average='micro', zero_division = 1)
            else: 
                rfecvDict[str(numVariables)]['f1'] = f1_score(gky_test, gky_pred, average='binary', zero_division = 1)

            print('Accuracy: ', rfecvDict[str(numVariables)]['accuracy'])
            print('Balanced Accuracy: ', rfecvDict[str(numVariables)]['balanced_accuracy'])
            print('F1 Score: ', rfecvDict[str(numVariables)]['f1'])

            # Get average variable importances across the five folds
            impts = [rfecvDict[str(numVariables)][str(f)]['var_impt'] for f in range(1, foldNum)]
            rfecvDict[str(numVariables)]['av_var_impt'] = np.mean(impts, 0)

            # Sort variables by average importance and drop the lowest n (gridStep)
            sorted_vars = sorted(zip(rfecvDict[str(numVariables)]['av_var_impt'], variables))
            variables = [x for _, x in sorted_vars][gridStep:]

        # Use scoring metric to choose optimal number of variables
        scores = [rfecvDict[str(numVariables)][scoring_metric] for numVariables in varSizeList]

        # Choose the top 5 max scores, then evaluate the moving window average around those scores to try to avoid picking a random spike.
        top_5_scores = sorted(range(len(scores)), key=lambda i: scores[i])[-5:]
        scores2 = [np.nan]+scores+[np.nan]
        moving_av = [np.nan]+[np.mean(scores2[i-1:i+1]) for i in range(2, len(scores2)-2)]+[np.nan]
        top_5_averages = [moving_av[i] for i in top_5_scores]
        max_index = top_5_scores[np.nanargmax(top_5_averages)]

        featureNumToSave = varSizeList[max_index]
        featuresToUse = rfecvDict[str(featureNumToSave)]['variables']

        print("Optimal number of features : %d" % featureNumToSave)

        # Plot number of features VS. cross-validation scores for each scoring metric
        for metric in ['accuracy', 'balanced_accuracy', 'f1']:
            y_vals = [rfecvDict[str(numVariables)][metric] for numVariables in varSizeList]
            fig = plt.figure()
            plt.xlabel("Number of features selected")
            plt.ylabel(metric+' score')
            plt.xlim(minFeatures, len(varSizeList)*gridStep)
            ax = plt.gca()
            ax.axvline(x=featureNumToSave, ymin=0, ymax=1, color = 'k', linestyle = ':')           
            plt.plot(varSizeList, y_vals)
            fig.savefig(os.path.join(variableSelectionOutdir, model+'_'+run_name+'_'+metric+'.png'))

        # Save Selected features and their feature importance scores to CSV:
        features = pd.DataFrame(np.column_stack((featuresToUse, rfecvDict[str(featureNumToSave)]['av_var_impt'])), columns = ['Variable Name', 'Variable Importance']).sort_values('Variable Importance', ascending = False)
        features.to_csv(os.path.join(variableSelectionOutdir, model+'_'+run_name+'_Variable_Selection_'+scoring_metric+'.csv'), index = False)

        # For use later:
        indNames = featuresToUse
        x = x[indNames]
        
    #--------------------------------------------------------------------------
    #                   Parameter Tuning
    #--------------------------------------------------------------------------
    if runParameterTuning:

        print('Running Parameter Tuning')
        print('model', model) 

        # Create dictionary of all possible parameters so GridSearchCV will output them in a useable way
        # Everything should be in an array, even if it's just one entry
        if len(indNames) > 32:
            maxF = 32
        else:
            maxF = len(indNames)
        userParams = {
            'n_estimators':             [30, 60, 100], # GEE breaks over 100 trees
            'criterion':                ['gini'],
            'max_depth':                [None],
            'min_samples_split':        [2], 
            'min_samples_leaf':         min_samples_leaf,
            'min_weight_fraction_leaf': [0],
            'max_features':             range(2,maxF,4),
            'max_leaf_nodes':           [None],
            'min_impurity_decrease':    [0],
            'bootstrap':                [True],
            'oob_score':                [True],
            'n_jobs':                   [n_jobs],
            'random_state':             [None],
            'verbose':                  [1],
            'warm_start':               [False],
            'class_weight':             [None]}

        estimator = RandomForestClassifier()

        start_time = datetime.now()
        with parallel_backend('threading'):
            grid_search = GridSearchCV(estimator, [userParams], cv = GroupKFold(5), scoring = scoring_metric, verbose = 0, n_jobs = n_jobs)
            grid_search.fit(x, y, groups)
        elapsed_time = datetime.now() - start_time
        print('Finished Running Grid Search. Elapsed time: ')
        print(elapsed_time)

        bestIndex = grid_search.best_index_
        bestScore = grid_search.cv_results_['mean_test_score'][bestIndex]
        print(grid_search.best_estimator_)
        print('Index of best estimator: ')
        print(bestIndex)
        print('Best ' + scoring_metric+': '+str(bestScore))

        # Do a run with best estimator to get accuracy statistics for tuning summary file
        rnd_clf = grid_search.best_estimator_
        rnd_clf.fit(x, y)
        print(vars(rnd_clf)) 
        print(dir(rnd_clf))  

        # Multiple classes:
        if multiclass:
            with parallel_backend('threading'):
                y_pred = cross_val_predict(rnd_clf, x, y, groups = groups, cv = GroupKFold(5))
                cross_validate_scores = cross_validate(rnd_clf, x, y, groups = groups, cv = GroupKFold(5), scoring = 'accuracy')
            print('Cross Validation Scores, '+scoring_metric+':')
            print(cross_validate_scores['test_score'])
        # Binary classes:
        else:
            print('Finding Threshold')
            with parallel_backend('threading'):
                y_probas_pred = cross_val_predict(rnd_clf, x, y, groups = groups, cv = GroupKFold(5), method = 'predict_proba')
            y_scores = y_probas_pred[:,1]
    
            # Choose best threshold and generate some stats
            #thisPrecision, thisRecall, thresholds = precision_recall_curve(y, y_scores, pos_label=None, sample_weight=None)

            thisF1 = []
            thisPrecision = []
            thisRecall = []
            thresholds = np.arange(0.1,0.51,0.01)
            for t in thresholds:
                y_pred = [1 if score >= t else 0 for score in y_scores] 
                thisF1.append(f1_score(y, y_pred, average = 'binary'))
                thisPrecision.append(precision_score(y, y_pred, average='binary'))
                thisRecall.append(recall_score(y, y_pred, average='binary'))

            f1 = thisF1[np.argmax(thisF1)]    
            precision = thisPrecision[np.argmax(thisF1)]   
            recall = thisRecall[np.argmax(thisF1)]
            threshold = thresholds[np.argmax(thisF1)]
            print("threshold:", threshold) 

            # Apply Threshold for confusion matrix 
            y_pred = (y_scores > threshold)

        # generate the rest of the stats
        cf_matrix = confusion_matrix(y, y_pred, labels = rnd_clf.classes_)
        df_cm = pd.DataFrame(cf_matrix, index = rnd_clf.classes_, columns = rnd_clf.classes_)
        kappa = cohen_kappa_score(y, y_pred)
        accuracy = accuracy_score(y, y_pred)
        balanced_accuracy = balanced_accuracy_score(y, y_pred)
        if not multiclass:
            roc_auc = roc_auc_score(y, y_pred)

        # Save parameters of best estimator, all stats, confusion matrix, and all variables to text file
        with open(estimatorParamFile, "w") as f:
            f.write(model+' TUNING OUTPUT\n\n')
            f.write('BEST ESTIMATOR PARAMETERS:\n')
            for param in grid_search.best_params_.keys():
                f.write(param+': '+str(grid_search.best_params_[param])+'\n')
            f.write('\n')
            if not multiclass:
                f.write('Best Threshold: '+str(threshold)+'\n')
                f.write('\n')
            f.write('Confusion Matrix: '+'\n')
            f.write(df_cm.to_string()+'\n')
            f.write('\n')
            f.write('Accuracy: '+str(accuracy)+'\n')
            f.write('Balanced Accuracy: '+str(balanced_accuracy)+'\n')
            f.write('Kappa: '+str(kappa)+'\n')
            if not multiclass:
                f.write('Precision: '+str(precision)+'\n')
                f.write('Recall: '+str(recall)+'\n')
                f.write('F1 Score: '+str(f1)+'\n')
                f.write('ROC/AUC: '+str(roc_auc)+'\n')
                f.write('\n')
            else:
                f.write(classification_report(y, y_pred))
            f.write('VARIABLES USED:\n')
            for name in indNames:
                f.write(name+'\n')

            # Print results
            print(model+' RESULTS')
            if not multiclass:
                print('Best Threshold: '+str(threshold))
            print('Confusion Matrix: ')
            print(cf_matrix)
            print('Accuracy: '+str(accuracy))
            print('Balanced Accuracy: '+str(balanced_accuracy))
            if not multiclass:
                print('Precision: '+str(precision))
                print('Recall: '+str(recall))
                print('F1 Score: '+str(f1))
            print('Kappa: '+str(kappa))

            # Save Tuning Parameters, Accuracy Statistics, and Variables Used to Summary File            
            run_description = run_name
            with open(summaryFile, 'w') as csvFile:
                csvwriter = csv.writer(csvFile, delimiter = ',', quoting = csv.QUOTE_MINIMAL)
                csvwriter.writerow(['Run Description', 
                                    'MTry/MaxFeatures/VarPerSplit', 
                                    'nTrees', 
                                    'minLeafPopulation', 
                                    'Best Threshold', 
                                    'Accuracy', 
                                    'Balanced Accuracy', 
                                    'F1 Score', 
                                    'Kappa', 
                                    'Variables'])
                
                if multiclass:
                    tuningResults = [run_name,
                            grid_search.best_params_['max_features'],
                            grid_search.best_params_['n_estimators'],
                            grid_search.best_params_['min_samples_leaf'],
                            'N/A',
                            accuracy,
                            balanced_accuracy,
                            'N/A',
                            kappa]
                else:
                    tuningResults = [run_name,
                            grid_search.best_params_['max_features'],
                            grid_search.best_params_['n_estimators'],
                            grid_search.best_params_['min_samples_leaf'],
                            threshold,
                            accuracy,
                            balanced_accuracy,
                            f1,
                            kappa]                     
                         
                for name in indNames:
                    tuningResults.append(name)                        
                csvwriter.writerow(tuningResults)
        compEndTime = datetime.utcnow()
        print('End Time (UTC): ', compEndTime)
        del rnd_clf, y_pred
        try:
            del grid_search, y_scores, min_samples_leaf, mtry, ntrees  
        except:
            continue
    
    
    
    #--------------------------------------------------------------------------
    #                   Learning Curves
    #--------------------------------------------------------------------------
    if plotLearningCurves:
        # Use parameters we just found.
        if runParameterTuning:
            estimator = grid_search.best_estimator_
            print('Plotting Learning Curves using: ')
            print(estimator)
        # Read in estimator parameters from file
        else: 
            if estimatorParamMethod == 'readFromFile':
                intParams = ['max_features', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split',
                                'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'verbose']
                with open(estimatorParamFile, 'r') as f:
                    lines = f.readlines()
                savedParams = {}
                for line in lines[3:19]:
                    sp = line.split(': ')
                    thisParameter = sp[1].replace('\n','')
                    if sp[0] in intParams:
                        savedParams[sp[0]] = int(thisParameter)
                    elif thisParameter == 'True':
                        savedParams[sp[0]] = True
                    elif thisParameter == 'False':
                        savedParams[sp[0]] = False
                    elif thisParameter == 'None':
                        savedParams[sp[0]] = None
                    else:
                        savedParams[sp[0]] = thisParameter
                estimator = RandomForestClassifier(**savedParams)
            elif estimatorParamMethod == 'default':
                estimator = RandomForestClassifier(n_estimators = 50, max_features = 'sqrt', n_jobs = n_jobs, oob_score = True, verbose = 0)

        #cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
        cv = 5
        train_sizes = np.linspace(.1, 1.0, 10)

        # Default Scoring
        plt, fig = accuracySamplingLib.plot_learning_curve(estimator, model+' Learning Curve - Accuracy', x, y, ylim=None, cv=cv,
                                n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'accuracy')
        fig.savefig(os.path.join(learningdir,'Learning_Curve_'+model+'_accuracy.png'))
        #plt.show()
        plt.close()

        # Balanced Accuracy
        plt, fig = accuracySamplingLib.plot_learning_curve(estimator, model+' Learning Curve - Balanced Accuracy', x, y, ylim=None, cv=cv,
                                n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'balanced_accuracy')
        fig.savefig(os.path.join(learningdir,'Learning_Curve_'+model+'_balanced_accuracy.png'))
        #plt.show()
        plt.close()

        if not multiclass:
            # F1
            plt, fig = accuracySamplingLib.plot_learning_curve(estimator, model+' Learning Curve - F1', x, y, ylim=None, cv=cv,
                                    n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'f1')
            fig.savefig(os.path.join(learningdir,'Learning_Curve_'+model+'_f1.png'))
            #plt.show()
            plt.close()

            # ROC/AUC
            plt, fig = accuracySamplingLib.plot_learning_curve(estimator, model+' Learning Curve - ROC/AUC', x, y, ylim=None, cv=cv,
                                    n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'roc_auc')
            fig.savefig(os.path.join(learningdir,'Learning_Curve_'+model+'_roc_auc.png'))
            #plt.show()
            plt.close()

            # # brier_score_loss
            # plt, fig = accuracySamplingLib.plot_learning_curve(estimator, model+' Learning Curve - Brier Score Loss', x, y, ylim=None, cv=cv,
            #                         n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'brier_score_loss')
            # fig.savefig(os.path.join(learningdir,'Learning_Curve_'+model+'_brier_score_loss.png'))
            # #plt.show()
            # plt.close()    
    



